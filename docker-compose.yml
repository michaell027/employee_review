services:
  ollama:
    build: ollama
    ports:
      - "11434:11434"
#    deploy:
#      resources:
#        reservations:
#          devices:
#          - driver: nvidia
#            capabilities: ["gpu"]
#            count: all
    networks:
      - llama
    entrypoint: ['usr/bin/bash', 'pull-llama3.sh']
#    restart: always

networks:
  llama:
    driver: bridge